
*********************************************************
***Python ml_test for APS method on 17-May-2021 23:55:18         ***
***    Reverse splitting is used!!      ***

*********************************************************
*** Experiemnt setup  ***
*** S(x,v) = 1/sqrt(2*pi)*v^2*e^{-v^2/2}*(1+cos(2*pi*(x+1/2))) ***
*** r(x) = ax+b, with a=[0.], b=[510.] and eps = 0.3  ***
*** Quantity of interest is F(X) = X  ***
*** No boundary conditions  ***
*********************************************************
Convergence tests, kurtosis, telescoping sum check 
*** using 120000 samples and 17 levels ***
 l dt^f mean(F(X^f)-F(X^c)) mean(F(X^c))  var(F(X^f)-F(X^c)) var(F(X^c)) cost(F(X^f)-F(X^c)) cost(F(X)) kurtosis consistency 
---------------------------------------------------------
0 1.0 0.0 0.4992975167366336 0.0 0.05845036338700574 0.0 5.420649999999582e-07 0.0 0.0
1 0.5 -0.00014021262126929823 0.5006510074639685 0.011945554180873588 0.056140016922997915 2.08899166666671e-06 5.929099999999466e-07 26.45849203792121 0.293330109466537
2 0.25 4.474369967491911e-05 0.5012243031769994 0.011047527932943111 0.053818152278022224 3.033506666666715e-06 7.350883333334223e-07 13.001346327015849 0.10632120465551118
3 0.125 -9.415718611199023e-05 0.4999722577146436 0.010255796989525579 0.052312634994097246 4.719832499999951e-06 1.128047499999892e-06 10.123233971755543 0.23791225751674996
4 0.0625 0.00022368189065934366 0.5007910078568469 0.009668004020003012 0.050757251107420964 8.812967500000054e-06 1.670563333333419e-06 7.337142045590275 0.12440282184855678
5 0.03125 9.023522622071042e-05 0.5013926451640466 0.008732254446401 0.050231367558197904 1.702635500000008e-05 2.9001341666666273e-06 6.125576676844205 0.10877797948290541
6 0.015625 0.0003983140604496209 0.4995698891739312 0.00818319156205552 0.04912523180161768 3.363822916666657e-05 5.310770000000057e-06 5.65656114802694 0.47828108996272467
7 0.0078125 -3.834334309495161e-05 0.5002924566190013 0.007476971259193978 0.04850289451972578 6.655466750000004e-05 1.0030386666666639e-05 5.344469411854022 0.1662973134947672
8 0.00390625 0.0002675405317287859 0.49987804543380965 0.006962286351558471 0.0475522749375262 0.0001332666616666667 1.917083083333324e-05 5.059013261362083 0.15092799069249813
9 0.001953125 2.482994678551602e-05 0.49952813018105263 0.0065277526151560576 0.0460837337764784 0.00026095698333333345 3.847105583333317e-05 4.805327971256145 0.0842634318339118
10 0.0009765625 0.00012323033565405172 0.49952754247444164 0.006224074567435267 0.04437944665401353 0.0005234770641666667 7.770641833333372e-05 4.728682304948789 0.028354765668072858
11 0.00048828125 -0.00020123909400192463 0.5009614958003871 0.005931646399871561 0.042928966935459556 0.0010291324633333334 0.00014948724833333316 4.799962357025665 0.38154267098527705
12 0.000244140625 -0.0001955265673254837 0.5000409484798275 0.005645137470007658 0.04166037451600868 0.002049596274166666 0.00028968015916666656 5.177576966811484 0.1721052670223957
13 0.0001220703125 0.0004258698066181325 0.5001634943643947 0.005077681260158307 0.040465779376827826 0.003985307739166667 0.0005927091808333335 5.865509804815863 0.07350009766749192
14 6.103515625e-05 0.0002468361892451771 0.5002923270556693 0.004476841227469934 0.040226282020015436 0.008051858454166666 0.001072394294999998 6.852252402172458 0.029075644867215047
15 3.0517578125e-05 -0.00016784025439827265 0.5002774143326361 0.0038850006646823283 0.0398765026704031 0.015913383176666667 0.002069154400000002 7.962122565118869 0.03817357589669314
16 1.52587890625e-05 8.676476136231886e-05 0.5002848811574725 0.0033509899370288967 0.03995560027143824 0.031178540906666666 0.0038323976775000005 9.22974823084334 0.0200157006266845

*********************************************************

*** Linear regression estimates of MLMC paramters ***

*** regression is done for levels with dt << eps^2 = 0.09 ***
*********************************************************
alpha = 0.7541863981787152 (exponent for weak convergence) 
beta = 0.2089468996871118 (exponent for variance of bias estimate) 
gamma = 0.976579849155981 (exponent for cost of bias estimate) 

*********************************************************
*** MLMC complexity test ***
*********************************************************
 e2 value mlmc_cost N_l dt 
---------------------------------------------------------
 0.01 0.47407024360163197 133.19009779999894 [328  16  16  16] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 0.005 0.47822444841928186 132.7990536000009 [248  16  16  16] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 0.0025 0.4887355870120487 132.6216212000004 [640  16  16  16] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 0.00125 0.4750247407660413 343.3188684000004 [4160   40   32   16   16] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05] 
 0.000625 0.4994283773770025 304.9635165999996 [6760   32   48   32] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 0.0003125 0.5123247751948214 875.5581328999981 [27440   272   152   112    72] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05] 
 0.00015625 0.5025203154691863 352.5545054000004 [19624   344   200   200] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 7.8125e-05 0.5063101486710478 5635.6832270999985 [195672   4976   2256    912    592    288    216] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05 5.51470588e-06 2.75735294e-06] 
 3.90625e-05 0.5008229183307661 1177.1593352000014 [172008   5168   2696   1240    432] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05] 
 1.953125e-05 0.5021229925821539 341.8481797000004 [49264  3856  2072  1640] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 9.765625e-06 0.501199544064197 523.8116659999996 [124728   7744   7608   5520] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 4.8828125e-06 0.49938606237923183 733.3510718000007 [195952  11032   7360   3368] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 2.44140625e-06 0.5000857117254459 8926.463451399999 [1918432  178376  114440   70376   30024    5904] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05 5.51470588e-06] 

