
*********************************************************
***Python ml_test for APS method on 17-May-2021 23:45:42         ***

*********************************************************
*** Experiemnt setup  ***
*** S(x,v) = 1/sqrt(2*pi)*v^2*e^{-v^2/2}*(1+cos(2*pi*(x+1/2))) ***
*** r(x) = ax+b, with a=[0.], b=[510.] and eps = 0.3  ***
*** Quantity of interest is F(X) = X  ***
*** No boundary conditions  ***
*********************************************************
Convergence tests, kurtosis, telescoping sum check 
*** using 120000 samples and 17 levels ***
 l dt^f mean(F(X^f)-F(X^c)) mean(F(X^c))  var(F(X^f)-F(X^c)) var(F(X^c)) cost(F(X^f)-F(X^c)) cost(F(X)) kurtosis consistency 
---------------------------------------------------------
0 1.0 0.0 0.497837079093625 0.0 0.10749293065235604 0.0 7.843266666666793e-07 0.0 0.0
1 0.5 -0.0006254258251938725 0.5010702574585226 0.02498897814918059 0.07927428794316255 1.0022716666667245e-06 6.221866666666169e-07 17.932133838114517 0.5805276016660603
2 0.25 -7.803616370799837e-05 0.49975909925780176 0.016294360089080508 0.06524010380727002 8.778950000000544e-07 8.110449999999621e-07 12.689474075650864 0.21423824288142154
3 0.125 0.0006395354619449581 0.5014424159077355 0.01233624212168846 0.05745087818753125 1.294690833333334e-06 1.1177183333334877e-06 9.623185930091239 0.19882812232784797
4 0.0625 0.0002801763170229585 0.5002455147344381 0.010208816558153734 0.05357846631521146 2.0773875000000904e-06 1.7875899999999352e-06 7.920711515410235 0.2980754985334279
5 0.03125 -0.0001293029589016429 0.49981560528625835 0.008805198995507926 0.051247683234154184 3.5961250000000316e-06 2.1805983333334092e-06 6.448929274657468 0.06291815006024835
6 0.015625 -2.3251465176549286e-05 0.5004537528985232 0.00799242924580588 0.04985088892305484 6.7421833333334294e-06 3.908178333333427e-06 5.955278841894082 0.14167768944148454
7 0.0078125 -0.0002600408906811055 0.4994189793805739 0.007287302111273786 0.04859275452890732 1.1193070833333371e-05 8.217705833333314e-06 5.4081511514928975 0.16908410024407922
8 0.00390625 -0.00014047514058443438 0.4993216062717432 0.006565063146273014 0.04757141436740761 2.106420499999994e-05 1.576584333333339e-05 5.055320313897445 0.009579036732862277
9 0.001953125 0.0002663102474020688 0.4996406135151143 0.005899885075456974 0.046296964420919656 4.228114916666677e-05 3.189291000000001e-05 4.842400121766727 0.011929196870252432
10 0.0009765625 0.0001651772804712248 0.49949315904565755 0.00537442967088814 0.044831735364972064 8.687995916666675e-05 6.181153083333329e-05 4.685368386247587 0.07216848711921114
11 0.00048828125 0.00024170892713622954 0.500888581850368 0.004616449168939876 0.042623201376374265 0.00017248992749999996 0.0001274702458333333 4.900248217444038 0.27403878607733845
12 0.000244140625 -0.0001097005426250771 0.4991695321061026 0.003960244057001803 0.04140931376517987 0.00032765881250000005 0.00024897326583333336 5.832642930581607 0.392980814721471
13 0.0001220703125 -0.00018221975106403688 0.5004650475103783 0.0032768319221259264 0.04037348404091351 0.0006321989858333332 0.0004892463966666666 7.660496208237853 0.3696036506185907
14 6.103515625e-05 0.0002143884792602235 0.5004586134106237 0.0027426038912122166 0.04030538897251751 0.001227204045 0.0009759989433333334 10.019262344933283 0.056156019175195936
15 3.0517578125e-05 0.00024056195853098235 0.5010663238653847 0.00227994501591746 0.03992249220320271 0.0024194061874999997 0.0019295598158333334 12.65033947488901 0.09456403286534777
16 1.52587890625e-05 8.362162092923044e-05 0.49927298451738467 0.0018786648804811688 0.03974733847108794 0.004789199486666666 0.0038544200633333333 15.370862513710938 0.4897727310668149

*********************************************************

*** Linear regression estimates of MLMC paramters ***

*** regression is done for levels with dt << eps^2 = 0.09 ***
*********************************************************
alpha = 0.6791397333462765 (exponent for weak convergence) 
beta = 0.2729192662128476 (exponent for variance of bias estimate) 
gamma = 0.9822046934016926 (exponent for cost of bias estimate) 

*********************************************************
*** MLMC complexity test ***
*********************************************************
 e2 value mlmc_cost N_l dt 
---------------------------------------------------------
 0.01 0.5364178673166818 8.860843400000078 [16 16 16 16] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 0.005 0.5320668309684851 9.067445700000292 [32 16 16 16] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 0.0025 0.49295992339249106 43.70707669999962 [288  32  32  16  16  16] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05 5.51470588e-06] 
 0.00125 0.5216121919328305 113.34509110000027 [1384   80   48   32   16   32   16] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05 5.51470588e-06 2.75735294e-06] 
 0.000625 0.48319652886505793 91.23311390000043 [2632   80   64   48   32   32] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05 5.51470588e-06] 
 0.0003125 0.511755529585331 65.12558950000027 [3800  160  136   80   48] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05] 
 0.00015625 0.489046242888481 20.891508700000312 [3632  328  152  160] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 7.8125e-05 0.49053882362840306 25.406225500000346 [6456  528  176  288] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 
 3.90625e-05 0.5005415276797643 102.75074749999975 [27552  1752   992   704   160] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05] 
 1.953125e-05 0.49536609738533943 125.78657559999982 [45224  4232  2184   920   352] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05] 
 9.765625e-06 0.501160366392897 310.1025162000001 [92752 12736  6704  3352  1856   792] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05 5.51470588e-06] 
 4.8828125e-06 0.5003639482348889 812.0453723999996 [385456  64920  42808  27328  14288   6720   1368] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05
 1.10294118e-05 5.51470588e-06 2.75735294e-06] 
 2.44140625e-06 0.5006142112785844 100.88588330000016 [97048 16208 10240  6432] [1.76470588e-04 8.82352941e-05 4.41176471e-05 2.20588235e-05] 

