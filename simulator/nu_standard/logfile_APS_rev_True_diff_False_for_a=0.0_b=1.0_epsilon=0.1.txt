
*********************************************************
***Python ml_test for APS method on 25-May-2021 18:39:47         ***
***    Reverse splitting is used!!      ***

*********************************************************
*** Experiemnt setup  ***
*** S(x,v) = 1/sqrt(2*pi)*v^2*e^{-v^2/2}*(1+cos(2*pi*(x+1/2))) ***
*** r(x) = ax+b, with a=[0.], b=[1.] and eps = 0.1  ***
*** Quantity of interest is F(X) = X  ***
*** No boundary conditions  ***
*********************************************************
Convergence tests, kurtosis, telescoping sum check 
*** using 180000 samples and 17 levels ***
 l dt^f mean(F(X^f)-F(X^c)) mean(F(X^c))  var(F(X^f)-F(X^c)) var(F(X^c)) cost(F(X^f)-F(X^c)) cost(F(X)) kurtosis consistency 
---------------------------------------------------------
0 1.0 0.0 0.4977556329076762 0.0 2.050119790530602 0.0 7.271344444444673e-07 0.0 0.0
1 0.5 -0.0029807664103061707 0.4951593996428555 0.7545000435169196 2.020262122466052 1.7290433333333194e-06 6.837044444444255e-07 7.149524696898611 0.014611520742362005
2 0.25 0.003295755906104417 0.5008047764849181 0.7504772560013507 2.0092755496651034 2.82767111111109e-06 7.552961111111342e-07 5.363987149790179 0.08968237178227335
3 0.125 -0.002002165513523027 0.5009993584272001 0.7157979205568892 1.9742792824787123 4.8318922222222625e-06 1.1132938888889122e-06 4.368887975447073 0.08468206592738595
4 0.0625 0.0002646075631529667 0.4927708395921938 0.6775541465255123 1.9156031774834308 8.423008333333312e-06 1.8400544444444635e-06 3.795137611121409 0.3325072559029421
5 0.03125 0.00044627308626092756 0.5049263850179002 0.6317633839718388 1.8512418190412507 1.692519777777777e-05 3.0645933333333e-06 3.5218619785011045 0.46784711828011116
6 0.015625 -0.0027482520775111937 0.5003867818157635 0.5881328037518431 1.7846425707126952 3.353019444444448e-05 5.854963888888894e-06 3.400868598423887 0.07314627486925342
7 0.0078125 -0.0028341172579045932 0.5006674380103481 0.5394651052180446 1.772814340939366 6.664756444444445e-05 1.0664084999999999e-05 3.4181337360175883 0.12948670563057504
8 0.00390625 0.0007425754870368396 0.5005557215356821 0.4561579409368536 1.808335835742189 0.00012981874444444448 1.8324039444444455e-05 3.5855483753394948 0.036046902058094916
9 0.001953125 -0.0015346716685347883 0.4988278581855783 0.3315092441374937 1.8752504353634076 0.00025991492444444453 3.6817115000000045e-05 3.995710615447827 0.008304613050673792
10 0.0009765625 -0.0006555815089538176 0.5011572894196581 0.21096129622206136 1.9339032255740478 0.0005133492777777777 7.082153444444449e-05 4.880974058939151 0.13112719833590847
11 0.00048828125 -0.0004687461288980617 0.5003170194477514 0.12159078476867227 1.9833018530237725 0.0010020175477777778 0.0001499187588888887 6.859785812942662 0.016692292444162978
12 0.000244140625 -0.00010879946747054205 0.4971935534346092 0.06452444496585855 1.98857713448939 0.0020049286199999997 0.0002541111327777778 10.732575433535354 0.1387601455468934
13 0.0001220703125 -0.0006386170460470673 0.503845769296669 0.0338211248525059 1.994075725154007 0.003929439811111112 0.0005397720055555558 17.478780098184696 0.3429852544293033
14 6.103515625e-05 0.0005118029102083065 0.494885865465586 0.017164313047862832 2.000125035020213 0.007799297157777777 0.0010435717894444446 30.167193145739233 0.4529340316977492
15 3.0517578125e-05 0.00025136254746729246 0.5013961525480797 0.008834323901643116 2.0057684652719545 0.015432688638333332 0.002037731299444445 71.97079198069697 0.3026655747013159
16 1.52587890625e-05 6.609499465426335e-05 0.5053024116986606 0.004243734070207493 2.015102437445613 0.030867231411111112 0.0038460037183333344 137.64781347664032 0.18720879995993

*********************************************************

*** Linear regression estimates of MLMC paramters ***

*** regression is done for levels with dt << eps^2 = 0.010000000000000002 ***
*********************************************************
alpha = 0.36763656112737736 (exponent for weak convergence) 
beta = 0.86277673560761 (exponent for variance of bias estimate) 
gamma = 0.9845205249386659 (exponent for cost of bias estimate) 

